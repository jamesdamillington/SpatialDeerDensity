{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Spatial Deer Distribution using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This notebook contains **Python code** and exercises associated with concepts discussed in lecture and [Millington _et al._ (2010)](http://dx.doi.org/10.1016/j.foreco.2009.12). The notebook _Modelling Spatial Deer Distribution using R_ provides the same examples but using R code. Compare the two notebooks to understand how the languages differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd               #for data\n",
    "import seaborn as sns             #for plotting\n",
    "import matplotlib.pyplot as plt   #for plotting\n",
    "import statsmodels.api as sm      #for regression modelling\n",
    "from scipy import stats           #for kendal tau\n",
    "from sklearn import linear_model  #for cross-validation\n",
    "from sklearn import model_selection\n",
    "import numpy as np                #for mean and variance \n",
    "import numpy.ma as ma             #for masked array\n",
    "import rasterio                   #for reading raster data\n",
    "from rasterio.plot import show    #for plotting raster data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Section 1: Loading and Checking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOI200 = pd.read_csv(\"data/LOI200.csv\") #read the file to a data.frame (assumes data are in data directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now view the first few lines of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOI200.head()                                 #view the first part of the data (to check it read properly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see a summary of the variables in the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOI200.describe()                              #view a summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Then we use some simple plotting functions to visualise the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#matrix scatter plot for 2nd to 6th columns (python is 0 indexed!)\n",
    "#no built-in function so use seaborn \n",
    "sns.pairplot(LOI200.iloc[:,1:6])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn scatter with axis labels\n",
    "sns.scatterplot(data=LOI200, x='DistanceLC', y='logDD')\n",
    "plt.xlabel(\"Distance LC (km)\")\n",
    "plt.ylabel(\"log(Deer Density)\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Correlation and Simple Linear Regression\n",
    "\n",
    "Now that we have familiarised ourselves with the data, we can do some simple correlations to examine the relationship of deer density with the distance to the nearest lowland conifer stand.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LOI200[['DistanceLC','logDD']].corr(method='pearson')) #calculate pearson correlation coefficient (r)\n",
    "print(LOI200[['DistanceLC','logDD']].corr(method='kendall')) #calculate kendall's correlation coefficient (tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We find some weak correlations (pandas does not give us an indication of statistical significance). \n",
    "\n",
    "The next piece of analysis we will do is to fit simple linear regression models to predict log(deer density) from environmental co-variates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use `linear_model` from the `sklearn` package to fit our simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = LOI200['DistanceLC']\n",
    "df_y = LOI200['logDD']                  \n",
    "\n",
    "new_df_X = df_X.values\n",
    "new_df_X = new_df_X.reshape(-1,1)\n",
    "\n",
    "#fit the model \n",
    "mod_dlc  = linear_model.LinearRegression()\n",
    "mod_dlc .fit(new_df_X, df_y)\n",
    "\n",
    "mod_dlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is no nice 'summary' method for `sklearn` `linear_model`s as there is in R. We can access model coefficients and intercept directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod_dlc.intercept_)\n",
    "print(mod_dlc.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But really this approach seems more suited to working with other computational tools for things like cross-validation (as we'll see below). For more straight-forward model fitting and analysis, we can use functions available in the `statsmodels` package. Read the docs and more detail [here](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html). Specifically we can use the `OLS` function from the `statsmodels.api`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `OLS` function from `statsmodels` to fit a regression requires we create an `OLS` object first, then use the `fit` [method](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.fit.html) on that object to create a `RegressionResults` [object](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html#statsmodels.regression.linear_model.RegressionResults). An easy way to create an `OLS` object is to use the `from_formula` [method](https://www.statsmodels.org/dev/dev/generated/statsmodels.base.model.Model.from_formula.html?highlight=from_formula#statsmodels.base.model.Model.from_formula) to pass the equation of the model we want to fit (as well as indicating what the data are that we are using):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires import statsmodels.api as sm\n",
    "mod_dlc = sm.OLS.from_formula(\"logDD ~ DistanceLC\", data = LOI200) \n",
    "mod_dlc_RR = mod_dlc.fit()\n",
    "print(mod_dlc_RR.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this model looks using a scatter plot of the variables - seaborn will fit a regression with confidence envelope for us (but note we have less control than in R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "g = sns.regplot(x=LOI200.DistanceLC, y=LOI200.logDD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "From the [statsmodel docs](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html) we can see that we can access the model residuals using `.resid`. We can use this to plot our own histogram of model residuals, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dlc_RR.resid.hist(bins=8)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This plot shows that the residuals are reasonably normally distributed and that we are likely not violating the assumptions of simple linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "R provides built-in diagnostic plotting functions for regression model objects. `statsmodels` in python doesn't offer this, but there are [relatively straight-forward ways to run diagnostics](https://www.statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To get some other useful model outputs, we need to get items from the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(mod_dlc_RR.params,3)    #directly access the model coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(mod_dlc_RR.rsquared,2)  #directly access the r.squared of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(mod_dlc_RR.pvalues,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this section has provided all we need to know to calculate the values in the first row of Table 1 in Millington et al. (2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.\n",
    "Use the code above to help you calculate values for the four other variables in Table 1 that have p < 0.1. Check you can get values from your code that correspond to those in Table 1 of Millington _et al._ (2010) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Section 3. Multiple linear regression models\n",
    "Fitting linear regression models with more than one are almost as straighforward as for simple (univariate) linear regression models. We just need to add the additional variables into the model equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dlc_dbh = sm.OLS.from_formula(\"logDD ~ DistanceLC + NewDBH\", data = LOI200) \n",
    "mod_dlc_dbh_RR = mod_dlc_dbh.fit()\n",
    "print(mod_dlc_dbh_RR.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To get a kendall correlation coefficient we need to output the predicted logDDs and then test using `kendalltau` from the [`stats` module of the `scipy` package](https://docs.scipy.org/doc/scipy/reference/stats.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dlc_dbh_RR_pred  = mod_dlc_dbh_RR.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires from scipy import stats \n",
    "stats.kendalltau(LOI200['logDD'], mod_dlc_dbh_RR_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Task 2.\n",
    "Use the code above to write your own code to calculate the values in Table 2 of Millington _et al._ (2010) for the Full Model (all values except for cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Cross-validation\n",
    "To complete the bottom two lines of Table 2 in Millington et al. (2010) we need to run cross-validation. In python, we can use functionality from the [`sklearn` package](https://scikit-learn.org/stable/index.html) for [cross-validation](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to fit the model again using `sklearn` to create a model object that it will be able handle:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires from sklearn import linear_model\n",
    "\n",
    "#create independent and dependent variables\n",
    "df_X = LOI200[['DistanceLC','NewDBH']]  \n",
    "df_y = LOI200['logDD']                  \n",
    "\n",
    "#fit the mode \n",
    "mod_dlc_dbh = linear_model.LinearRegression()\n",
    "mod_dlc_dbh.fit(df_X, df_y)\n",
    "\n",
    "#we can get simple otuputs\n",
    "print(mod_dlc_dbh.coef_)  #model coefficients\n",
    "print(mod_dlc_dbh.score(df_X, df_y)) #r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the fitted model to estimate predicted values for all out observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Predicted = mod_dlc_dbh.predict(df_X)\n",
    "print(Predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a single 5-fold cross validation for the regression model we would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires from sklearn import model_selection\n",
    "cvpred = model_selection.cross_val_predict(mod_dlc_dbh, df_X, df_y, cv=5)\n",
    "print(cvpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has created an array of predicted values based on models fit for each of the folds created (see `model_selection.KFold` for [more](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=kfold#sklearn.model_selection.KFold) on how the folds are created). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now append the predictors (from both all observations and the cross-validation) to the original data to allow us to calculate correlations for comparing between using all observed data vs k-folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVdata = LOI200\n",
    "CVdata['Predicted'] = Predicted\n",
    "CVdata['cvpred'] = cvpred\n",
    "\n",
    "#Predicted\n",
    "r = CVdata['logDD'].corr(CVdata['Predicted'])\n",
    "r2 = r**2\n",
    "t = CVdata['logDD'].corr(CVdata['Predicted'], method='kendall')\n",
    "    \n",
    "print(\"r2 (obs): \", round(r2,3))   \n",
    "print(\"tau (obs): \", round(t,3))\n",
    "print('\\n')\n",
    "\n",
    "#Cross Validated\n",
    "r = CVdata['logDD'].corr(CVdata['cvpred'])\n",
    "r2 = r**2\n",
    "t = CVdata['logDD'].corr(CVdata['cvpred'], method='kendall')\n",
    "    \n",
    "print(\"r2 (cv): \", round(r2,3))   \n",
    "print(\"tau (cv): \", round(t,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But note how the caption of Table 2 indicates that \n",
    ">\"estimates for cross-validation results are 95% confidence intervals calculated from mean and variance of 100 repetitions.\" \n",
    "\n",
    "To do this we need to create a loop to run the cross-validation multiple times, storing the results so we can calculate the mean\n",
    "and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_r2 = []\n",
    "cv_tau = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    kf = model_selection.KFold(n_splits=5,shuffle=True, random_state=i)  #needed to create random folds (using i)\n",
    "    cvpred = model_selection.cross_val_predict(mod_dlc_dbh, df_X, df_y, cv=kf)  #predict using current random folds\n",
    "    \n",
    "    #correlations\n",
    "    r = LOI200['logDD'].corr(pd.Series(cvpred))\n",
    "    tau = LOI200['logDD'].corr(pd.Series(cvpred), method='kendall')\n",
    "    \n",
    "    #append to list\n",
    "    cv_r2.append(r**2)\n",
    "    cv_tau.append(tau)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate mean and variance for r2 and tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cv_r2))\n",
    "print(np.mean(cv_tau))\n",
    "print(1.96 * np.var(cv_r2))  \n",
    "print(1.96 * np.var(cv_tau))\n",
    "\n",
    "#printoptions precision does not work for numpy.float64! \n",
    "#see https://stackoverflow.com/questions/47637972/numpy-set-printoptions-precision-ignored-in-list-of-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.\n",
    "\n",
    "Build on the code above to calculate the cross-validation values for the 'Full' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Section 5. Spatial Estimation\n",
    "Later in Millington _et al._ (2010) models fit from the sample data (at 51 locations) were used to predict deer density across a subsection of the study area. We readily do this in R using the functionality in the `raster` package. Things are not so straight-forward in python because the raster processing tools do not seem to be so well developed... \n",
    "\n",
    "In python, the main package for raster tools seems to be [`rasterio`](https://rasterio.readthedocs.io/). However, the rasterio package doesn't have predict methods like for R and to do what we want to do here, [others have suggested simply using R!](https://stackoverflow.com/questions/48853484/how-to-structure-data-for-prediction-using-rasters-as-targets). As its name suggests, rasterio is focused on raster input and output.  \n",
    "\n",
    "After some exploring, I thought we might be able use [pyimpute](https://github.com/perrygeo/pyimpute) which has some nice features and contains a lot of the functionality we want. _But_, pyimpute assumes the data used to fit models are from the study rasters themselves (indicated by points within the raster). In the current situation, we have already fit the model using point data and now simply want to apply the fitted model parameters to our explanatory rasters.\n",
    "\n",
    "Further googling suggested we could use `numpy arrays` with the numpy predict methods, for example see [here](https://stackoverflow.com/questions/29036179/what-is-the-python-equivalent-to-r-predict-function-for-linear-models) and [here](https://www.statsmodels.org/devel/examples/notebooks/generated/predict.html). We would have to do some fiddling moving back and forth between 1D arrays and 2D rasters but it should be possible. To specify the point location values we will need to use [numpy masked arrays](https://numpy.org/doc/stable/reference/maskedarray.generic.html) (e.g. see [here](https://stackoverflow.com/a/38856546) and [here](https://stackoverflow.com/a/38194439))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "First, we'll do a uni-variate model for distance to lowland conifer. Let's view the raster data we're going to use to predicte deer distribution (distance to lowland conifer).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the raster with rasterio,  requires import rasterio and from rasterio.plot import show\n",
    "dlc_r = rasterio.open('data/LOI200_dlc_km.asc')\n",
    "rasterio.plot.show(dlc_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Note the 'empty' block (no data) in the bottom left. This will be important in a minute...\n",
    "\n",
    "Next, we create [a 1D `ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) from this 2D raster: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the raster with rasterio,  requires import rasterio\n",
    "#we need to use src.read for flatten() method below\n",
    "with rasterio.open('data/LOI200_dlc_km.asc') as src:\n",
    "    dlc_r = src.read(1)  \n",
    "    \n",
    "dlc_f = dlc_r.flatten()  #flatten from 2d to 1D\n",
    "\n",
    "#check we have produced a 1D array\n",
    "print(type(dlc_f))\n",
    "print(dlc_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that block of no data in the bottom left of the raster? We tell python to ignore this. We do that using a masked array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the mask. requres import numpy.ma as ma\n",
    "ma_dlc = ma.masked_values(dlc_f, -9999)   #specify that -9999 is no data \n",
    "mac_dlc = ma.compressed(ma_dlc)           #remove these no data values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `mod_dlc` sklearn linear model object we created above to make our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model \n",
    "mod_dlc  = linear_model.LinearRegression()\n",
    "mod_dlc .fit(new_df_X, df_y)\n",
    "\n",
    "res = mod_dlc.predict(dlc_f.reshape(-1,1))\n",
    "res = res.astype(np.float32) #original were float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we convert our 1D data back to 2D we need re-insert the no data values we removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dlc = ma.MaskedArray(res, mask=ma_dlc.mask,fill_value=-9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can convert back to a 2D map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2D = pred_dlc.reshape(dlc_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterio.plot.show(pred2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! What took a couple of lines in R, took a lot more work in python. And getting a legend on that plot will take [even more messing with matplotlib](https://stackoverflow.com/questions/25482876/how-to-add-legend-to-imshow-in-matplotlib). We can at least just make the plot a little larger...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "im = ax.imshow(pred2D)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Task 4.\n",
    "\n",
    "Use code in this section to create a predicted deer density map for the 'best model' (i.e. using `LOI200_dlc_km.asc` and `LOI200_Meandbh_cm.asc` as predictor maps).\n",
    "\n",
    "_Hint:_ to predict a model across multiple `np array`s you will need to [stack](https://stackoverflow.com/a/48818880) the arrays into a single array object (then `predict` on that object). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
